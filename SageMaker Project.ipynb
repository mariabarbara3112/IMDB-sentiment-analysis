{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Introduction:\n",
    "    \n",
    "The goal is to predict a sentiment from film reviews, to construct and deploy an inference rule(endpoint) and create an App to interact with the models endpoint.\n",
    "\n",
    "After some preprocessing and splitting into training,  validation and test set the data is uploaded to Amazons S3 storage structure. Then the training data is used to train a random tree model (xgboost).\n",
    "\n",
    "The trained model might suffer from under-or overfitting (also called bias-variance-tradeoff), though. To determine the accuracy of the model's pred we have to predict new data; this new data is the part of the dataset that was packed into the test set file. In between the model has been optimized with some hyperparameter tuning, for which another dataset is used which has been put aside during preprocessing.\n",
    "\n",
    "Definitions:\n",
    "0:=negative review\n",
    "1:=positive review\n",
    "    \n",
    "The dataset that is used has been provided by:\n",
    "    \n",
    "Maas, Andrew L., et al. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2011.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1: Downloading the data\n",
    "\n",
    "As in the XGBoost in SageMaker notebook, we will be using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Preparing and Processing the data\n",
    "- read in each of the reviews and combine them into a single input structure\n",
    "- split the dataset into a training set and a testing set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def loadreviews(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    ratings = {}\n",
    "    \n",
    "    for setmembership in ['train', 'test']:\n",
    "        data[setmembership] = {}\n",
    "        ratings[setmembership] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[setmembership][sentiment] = []\n",
    "            ratings[setmembership][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, setmembership, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[setmembership][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    ratings[setmembership][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[setmembership][sentiment]) == len(ratings[setmembership][sentiment]), \\\n",
    "                    '{}/{} data size does not match ratings size'.format(setmembership, sentiment)\n",
    "                \n",
    "    return data, ratings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "data, ratings = loadreviews()\n",
    "print('IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg'.format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#combine the positive and negative reviews and shuffle the resulting records:\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffledata(data, ratings):\n",
    "    '''Prepare training and test sets from IMDb movie reviews.'''\n",
    "    \n",
    "    #Combine positive and negative reviews and ratings\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    ratings_train = ratings['train']['pos'] + ratings['train']['neg']\n",
    "    ratings_test = ratings['test']['pos'] + ratings['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding ratings within training and test sets\n",
    "    data_train, ratings_train = shuffle(data_train, ratings_train)\n",
    "    data_test, ratings_test = shuffle(data_test, ratings_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training ratings, test labets\n",
    "    return data_train, data_test, ratings_train, ratings_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "xtrain, xtest, ytrain, ytest = shuffledata(data, ratings)\n",
    "print('IMDb reviews (combined): train = {}, test = {}'.format(len(xtrain), len(xtest)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#check and see an example of the data our model will be trained on:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(xtrain[100])\n",
    "print(ytrain[100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#make sure that any html tags are removed\n",
    "#tokenize input (example: words such as entertained and entertaining are considered the same)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def reviewtowords(review):\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    vote = Porterstemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, 'html.parser').get_text() \n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower()) \n",
    "    strings = text.split() \n",
    "    strings = [x for x in strings if x not in strings] \n",
    "    \n",
    "    return strings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Question: The `reviewtowords` method removes html tags and converts the reviews into a tokenized format. \n",
    "# preprocess_data removes stopwords like 'the' and caches the results (which allows splitting the analysis into several parts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "reviewtowords(xtrain[100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "cdir = os.path.join('../cache', 'sentiment_analysis')  # where to store cache files\n",
    "os.makedirs(cdir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocessdata(data_train, data_test, ratings_train, ratings_test,\n",
    "                    cdir=cdir, cfl='preprocessed_data.pkl'):\n",
    "    ''''''Convert each review to words; read from cache if available.''''''\n",
    "\n",
    "    # If cfl is not None, try to read from it first:\n",
    "    cd = None\n",
    "    if cfl is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cdir, cfl), 'rb') as f:\n",
    "                cd = pickle.load(f)\n",
    "            print('Read preprocessed data from cache file:', cfl)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing:\n",
    "    if cd is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #stringstrain = list(map(reviewtowords, data_train))\n",
    "        #stringstest = list(map(reviewtowords, data_test))\n",
    "        \n",
    "        print('[INFO] Preprocessing Train Data...')\n",
    "        stringstrain = [reviewtowords(review) for review in tqdm(data_train)]\n",
    "        \n",
    "        print('[INFO] Preprocessing Test Data...')\n",
    "        stringstest = [reviewtowords(review) for review in tqdm(data_test)]\n",
    "        \n",
    "        print('[INFO] Caching data to file...')\n",
    "        # Write to cache file for future runs\n",
    "        if cfl is not None:\n",
    "            cd = dict(stringstrain=stringstrain, stringstest=stringstest,\n",
    "                              ratings_train=ratings_train, ratings_test=ratings_test)\n",
    "            with open(os.path.join(cdir, cfl), 'wb') as f:\n",
    "                pickle.dump(cd, f)\n",
    "            print('Wrote preprocessed data to cache file:', cfl)\n",
    "    else:\n",
    "        stringstrain, stringstest, ratings_train, ratings_test = (cd['stringstrain'],\n",
    "                cd['stringstest'], cd['ratings_train'], cd['ratings_test'])\n",
    "    \n",
    "    return stringstrain, stringstest, ratings_train, ratings_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocess data\n",
    "xtrain, xtest, ytrain, ytest = preprocess_data(xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "xtrain[:2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Further steps:\n",
    "- construct a feature representation which represents each word as an integer; only include the words that appear most frequently. \n",
    "- combine all of the rareworduent words into a single category and label it as `1`.\n",
    "- convert all reviews to the same size: for that fix a size and fill short reviews with the category 'no word' (`0`) and truncate long reviews."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: Create a word dictionary\n",
    "\n",
    "To begin with, we need to construct a way to map words that appear in the reviews to integers. Here we fix the size of our vocabulary (including the 'no word' and 'rareworduent' categories) to be `5000` but you may wish to change this to see how it affects the model.\n",
    "\n",
    "TODO: Complete the implementation for the `build_dict()` method below. Note that even though the vocab_size is set to `5000`, we only want to construct a mapping for the most frequently appearing `4998` words. This is because we want to reserve the special ratings `0` for 'no word' and `1` for 'rareworduent word'."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def build_dict(reviews, vocab_size = 5000):\n",
    "    \n",
    "    strings = [x for rev in reviews for x in rev]\n",
    "    \n",
    "    strcount = dict(Counter(strings)) # A dict storing the words that appear in the reviews along with how often they occur\n",
    "\n",
    "    sortedstrings = sorted(Counter(strings), key=Counter(strings).get, reverse=True)\n",
    "    \n",
    "    strdict = {} # dictionary that translates words into integers\n",
    "    for idx, x in enumerate(sortedstrings[:vocab_size - 2]): # The -2 is to accomodate 'noword' and 'rareword' category\n",
    "        strdict[x] = idx + 2                              \n",
    "        \n",
    "    return strdict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "QUESTION:\n",
    "What are the five most frequently appearing (tokenized) words in the training set? Does it makes sense that these words appear frequently in the training set?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "['movi', 'film', 'one', 'like', 'time']`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "strdict = build_dict(xtrain)\n",
    "list(strdict)[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import os\n",
    "datadir = '../data/pytorch' # storage\n",
    "if not os.path.exists(datadir):\n",
    "    os.makedirs(datadir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "with open(os.path.join(datadir, 'strdict.pkl'), 'wb') as file:\n",
    "    pickle.dump(strdict, file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Transform the reviews into integer sequence representation, making sure to fill or truncate to length 500 (here)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def convert_and_fill(strdict, sentence, fill=500):\n",
    "    noword = 0 #occuring never\n",
    "    rareword = 1 #words not appearing in strdict\n",
    "    validsentence = [noword] * fill\n",
    "    for wordidx, word in enumerate(sentence[:fill]):\n",
    "        if word in strdict:\n",
    "            validsentence[wordidx] = strdict[word]\n",
    "        else:\n",
    "            validsentence[wordidx] = rareword\n",
    "    return validsentence, min(len(sentence), fill)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "xtrain, xtrainlen = convert_and_fill_data(word_dict, xtrain)\n",
    "xtest, xtestlen = convert_and_fill_data(word_dict, xtest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#check that everything is still working by showing some of the data\n",
    "\n",
    "xtest[:2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "QUESTION: In the cells above we use the `preprocess_data` and `convert_and_fill_data` methods to process both the training and testing set. Why or why not might this be a problem?\n",
    "\n",
    "The first method tokenizes words and removes html tags, stopwords and other formatting code, while the second method converts the reviews to a fixed length by adding zeros or truncating the data matrix; rareworduent words are replaced by 1. As training, validation and test set all use the same type of data from the same source these methods can be applied to all 3 sets.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''Upload the data to S3\n",
    "First save it locally and we will upload to S3 later on; each row of the dataset has the form `label`, `length`, `review[500]` where `review[500]` is a sequence of `500` integers for the word counts in a review.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "    \n",
    "pd.concat([pd.DataFrame(ytrain), pd.DataFrame(xtrain_len), pd.DataFrame(xtrain)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#upload the training data to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "input = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#build and train a Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "!pygmentize train/model.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "There are three parameters to be optimized; the\n",
    "- embedding dimension\n",
    "- hidden dimension\n",
    "- size of the vocabulary\n",
    "and make these parameters configurable in the training script.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# load a small portion of the training data set (no GPU available)\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Read in only the first 250 rows\n",
    "training = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "ytraining = torch.from_numpy(training[[0]].values).float().squeeze()\n",
    "xtraining = torch.from_numpy(training.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "trainingtensor = torch.utils.data.TensorDataset(xtraining, ytraining)\n",
    "\n",
    "# Build the dataloader\n",
    "trainingloaded = torch.utils.data.DataLoader(trainingtensor, batch_size=50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#TODO Writing the training method"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch_X, batch_y = batch\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            # zero accumulated gradients\n",
    "            model.zero_grad()\n",
    "             # get the output from the model\n",
    "            output = model(batch_X)\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = loss_fn(output.squeeze(), batch_y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data.item()\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#test that it is working on the small sample training set loaded earlier:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import torch.optim as optim\n",
    "from train.model import LSTMClassifier\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, trainingloaded, 5, optimizer, loss_fn, device)\n",
    "\n",
    "model = device.RNNModelScratch(32, 100, 5000).to(device)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "''''''\n",
    "TODO Training the model\n",
    "Inside of the `train` directory is a file called `train.py` which has been provided and which contains most of the necessary code to train the model. The only thing that is missing is the implementation of the `train()` method which you wrote earlier in this notebook.\n",
    "\n",
    "TODO: Copy the `train()` method written above and paste it into the `train/train.py` file where required.\n",
    "The way that SageMaker passes hyperparameters to the training script is by way of arguments. These arguments can then be parsed and used in the training script. To see how this is done take a look at the provided `train/train.py` file.\n",
    "''''''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='1.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 20,\n",
    "                        'hidden_dim': 256,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "estimator.fit({'training': input})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model_data = estimator.model_data\n",
    "with open(os.path.join(data_dir,'model_data.path'),'wb') as f:\n",
    "    pickle.dump(model_data,f)\n",
    "print(model_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "''''''Testing the model\n",
    "-deploy model \n",
    "- send the test data to the deployed endpoint to check if everything is working\n",
    "\n",
    "Deploy the model for testing\n",
    "Currently the model takes input of the form `review_length, review[500]`; `review[500]` is a sequence of `500` integers which describe the words present in the review, encoded using `word_dict`.\n",
    "\n",
    "- write function which loads the saved model. This function must be called `model_fn()` and takes as its only parameter a path to the directory where the model artifacts are stored. This function must also be present in the python file that was created when deploying/testing the model(which was already done above)\n",
    "\n",
    "TODO: Deploy the trained model.\n",
    "''''''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "predictor = estimator.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# use model for testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "xtest = pd.concat([pd.DataFrame(xtest_len), pd.DataFrame(xtest)], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#split the data into batches and send only one batch at a time\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in tqdm(split_array):\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pred = predict(xtest.values)\n",
    "pred = [round(x) for x in pred]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "''''''\n",
    "\n",
    "Question: \n",
    "How does this model compare to the XGBoost model you created earlier? Why might these two models perform differently on this dataset? Which do *you* think is better for sentiment analysis\n",
    "\n",
    "Random Tree model: 86% accuracy on the test set \n",
    "RNN model: 84.3% accuracy on the test set\n",
    "\n",
    "Neural Network models have more parameters (weights and biases) than random tree models like xgb boost, so they tend to 'overfit', that is have a worse generalization on new (test) data. In the RNN model the parameters are called weights and biases for which error gradients are calculated (how much a change in each weight changes the total error; the adaptations that are made are proportional to the size of their influence on the total error) while the xgb boost model creates different random tree structures to minimize the total error of the predictions. To perform better artificial neural networks have to run over a long time and get much more data than in the review database / the small sample that was downloaded above due to computational capacity / cost constraints (GPUs are much more expensive than CPUs).\n",
    "\n",
    "''''''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "xtest[:2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#TODO: More testing: send an unprocessed review as a string"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test_review = 'As I write this, I want to describe my raw initial state after I finished the film, I'm in a state of complete awe, staring into the wall kind of awe, Parasite is truly a work of art, a sheer masterpiece. This film oozes with mastery, every little detail tells a story of its own, I was drawn to it like a moth to a flame, it grips hard and it never lets go, it sways between genres gracefully, it offers comedy both dark and light, drama, horror, thrill, and it's all packaged so seamlessly, conveyed to us throughout breathtaking performances across the board.'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#TODO: Using the `reviewtowords` and `convert_and_fill` methods from section one, convert `test_review` into a numpy array `test_data` suitable to send to our model. Remember that our model expects input of the form `review_length, review[500]`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Convert test_review into a form usable by the model and save the results in test_data\n",
    "testtokens = reviewtowords(testreview)\n",
    "testsentence, testdatalen = convert_and_fill(worddict,testtokens)\n",
    "\n",
    "datapack = np.hstack((test_data_length, test_sentence))\n",
    "testdata = datapack.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#send the resulting array to the model to predict the sentiment of the review."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "predictor.predict(testdata)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the return value is close to `1` the review is positive."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Delete the endpoint"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "''''''\n",
    "Deploy the model for the app\n",
    "\n",
    "To do:\n",
    "- make interface so the user input can be sent to the model; it is also necessary to provide further code so the input gets the preprocessing required by the model(estimator)\n",
    "- after deployment the estimator (model initializer) storesthe results in the directory we specified when deploying the endpoint; in this case it will be stored in the 'serve' directory.\n",
    "\n",
    "In this directory there have to be the following files: \n",
    "-`model.py` file that we used to construct our model, \n",
    "- a `utils.py` file which contains the `reviewtowords` and `convert_and_fill` pre-processing functions \n",
    "-`predict.py`, the file which will contain the inference code. \n",
    "-`requirements.txt` which will tell SageMaker what Python libraries are required by the inference code.\n",
    "\n",
    "When deploying a PyTorch model it is necessary to provide the following functions for theinference container, namely:\n",
    " - `model_fn`: This function is the same function that we used in the training script and it tells SageMaker how to load our model.\n",
    " - `input_fn`: This function receives the raw serialized input that has been sent to the model's endpoint and its job is to de-serialize and make the input available for the inference code.\n",
    " - `output_fn`: This function takes the output of the inference code and its job is to serialize this output and return it to the caller of the model's endpoint.\n",
    " - `predict_fn`: The heart of the inference script, this is where the actual prediction is done and is the function which you will need to complete.\n",
    "\n",
    "The `input_fn` and `output_fn` methods have to be able to accept a string as input and return a single value as output. \n",
    "\n",
    "(TODO) Writing inference code\n",
    "Before writing our custom inference code, we will begin by taking a look at the code which has been provided.\n",
    "''''''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "!pygmentize serve/predict.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "''''''\n",
    "The `model_fn` method is the same as the one provided in the training code and the `input_fn` and `output_fn` methods are very simple.\n",
    "\n",
    "Make sure that you save the completed file as `predict.py` in the `serve` directory.\n",
    "\n",
    "TODO: Complete the `predict_fn()` method in the `serve/predict.py` file.\n",
    "''''''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "''''''\n",
    "Deploying the model\n",
    "\n",
    "Now that the custom inference code has been written, we will create and deploy our model. To begin with, we need to construct a new PyTorchModel object which points to the model artifacts created during training and also points to the inference code that we wish to use. Then we can call the deploy method to launch the deployment container.\n",
    "\n",
    "\n",
    "A deployed PyTorch model requires the input passed to the predictor to be a numpy array; as in this case a string is passed it is necessary to construct a wrapper around the `RealTimePredictor`. \n",
    "(In a more complicated situation you may want to provide a serialization object, for example if you wanted to sent image data.)\n",
    "''''''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "with open(os.path.join(data_dir,'model_data.path'), 'rb') as f:\n",
    "    model_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Testing the model:\n",
    "Load first 250 reviews and send them to the endpoint and collect the results.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "xtest = pd.concat([pd.DataFrame(xtestlength), pd.DataFrame(xtest)], axis=1)\n",
    "def predict(input, rows=512):\n",
    "    splitarray = np.array_split(input, int(input.shape[0] / float(rows) + 1))\n",
    "    pred = np.array([])\n",
    "    for s in splitarray:\n",
    "        pred = np.append(pred, predictor.predict(s))\n",
    "    \n",
    "    return pred\n",
    "\n",
    "pred = predict(xtest.values)\n",
    "pred = [round(x) for x in pred]\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(testy, predictions)\n",
    "\n",
    "testrev = 'The simplest pleasures in life are the best, and this film is one of them'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import glob\n",
    "\n",
    "def test_reviews(data_dir='../data/aclImdb', stop=250):\n",
    "    results = []\n",
    "    ground = []\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
    "        files = glob.glob(path)\n",
    "        files_read = 0\n",
    "        print('Starting ', sentiment, ' files')\n",
    "        # Iterate through the files and send them to the predictor\n",
    "        for f in files:\n",
    "            with open(f) as review:\n",
    "                # First, we store the ground truth (was the review positive or negative)\n",
    "                if sentiment == 'pos':\n",
    "                    ground.append(1)\n",
    "                else:\n",
    "                    ground.append(0)\n",
    "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
    "                review_input = review.read().encode('utf-8')\n",
    "                # Send the review to the predictor and store the results\n",
    "                result = float(predictor.predict(review_input))\n",
    "                results.append(round(result))\n",
    "            files_read += 1\n",
    "            if files_read == stop:\n",
    "                break\n",
    "    return ground, results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ground, results = test_reviews()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ground, results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As an additional test, try sending the `test_review` from above:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "predictor.predict(test_review)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "''''''\n",
    "Now that we know our endpoint is working as expected, we can set up the web page that will interact with it. If you don't have time to finish the project now, make sure to skip down to the end of this notebook and shut down your endpoint. You can deploy it again when you come back.\n",
    "''''''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "''''''\n",
    "Use the model in the app\n",
    "\n",
    "> TODO: This entire section and the next contain tasks for you to complete, mostly using the AWS console.\n",
    "\n",
    "So far we have been accessing our model endpoint by constructing a predictor object which uses the endpoint and then just using the predictor object to perform inference. What if we wanted to create a web app which accessed our model? The way things are set up currently makes that not possible since in order to access a SageMaker endpoint the app would first have to authenticate with AWS using an IAM role which included access to SageMaker endpoints. However, there is an easier way! We just need to use some additional AWS services.\n",
    "\n",
    "<img src='Web App Diagram.svg'>\n",
    "\n",
    "-construct a Lambda function, which you can think of as a straightforward Python function that can be executed whenever a specified event occurs; give this function permission to send and receive data from a SageMaker endpoint.\n",
    "\n",
    "-to execute the Lambda function create a new endpoint using API Gateway. This endpoint will be a url that listens for data to be sent to it. Once it gets some data it will pass that data on to the Lambda function and then return whatever the Lambda function returns. Essentially it will act as an interface that lets our web app communicate with the Lambda function.\n",
    "\n",
    "- Setup a Lambda function\n",
    "This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the review) to the SageMaker endpoint we've created and then return the result.\n",
    " - Create an IAM Role for the Lambda function\n",
    "   -make sure that it has permission to do so. To do this construct a rolefor the Lambda function.\n",
    "    Using the AWS Console, navigate to the **IAM** page and click on 'Roles'. Then, click on **Create role**. Make sure that \n",
    "    the **AWS service** is the type of trusted entity selected and choose 'Lambda** as the service that will use this role,         then click **Next: Permissions**.\n",
    "\n",
    "In the search box type `sagemaker` and select the check box next to the 'AmazonSageMakerFullAccess' policy. Then, click on ''Next: Review**.\n",
    "\n",
    "Lastly, give this role a name. Make sure you use a name that you will remember later on, for example `LambdaSageMakerRole`. Then, click on **Create role**.\n",
    "\n",
    "- Create a Lambda function\n",
    "Using the AWS Console, navigate to the AWS Lambda page and click on **Create a function**. When you get to the next page, make sure that 'Author from scratch' is selected. Now, name your Lambda function, using a name that you will remember later on, for example `sentiment_analysis_func`. Make sure that the **Python 3.6** runtime is selected and then choose the role that you created in the previous part. Then, click on **Create Function**.\n",
    "\n",
    "On the next page you will see some information about the Lambda function you've just created. If you scroll down you should see an editor in which you can write the code that will be executed when your Lambda function is triggered. In our example, we will use the code below.\n",
    "''''''\n",
    "\n",
    "```python\n",
    "\n",
    "# use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda\n",
    "\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT NAME HERE**',\n",
    "                                       ContentType = 'text/plain',\n",
    "                                       Body = event['body'])\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : result\n",
    "    }\n",
    "```\n",
    "\n",
    "''''''\n",
    "Once you have copy and pasted the code above into the Lambda code editor, replace the `**ENDPOINT NAME HERE**` portion with the name of the endpoint that we deployed earlier. You can determine the name of the endpoint using the code cell below.\n",
    "''''''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once you have added the endpoint name to the Lambda function, click on Save. Your Lambda function is now up and running. Next we need to create a way for our web app to execute the Lambda function.\n",
    "\n",
    "Setting up API Gateway\n",
    "create a new API using API Gateway that will trigger the Lambda function.\n",
    "\n",
    "Using AWS Console, navigate to Amazon API Gateway and then click on Get started.\n",
    "\n",
    "On the next page, make sure that New API is selected and give the new api a name, for example, `sentiment_analysis_api`. Then, click on Create API.\n",
    "\n",
    "Select the Actions dropdown menu and click Create Method. A new blank method will be created, select its dropdown menu and select POST, then click on the check mark beside it.\n",
    "\n",
    "For the integration point, make sure that Lambda Function is selected and click on the 'Use Lambda Proxy integration'. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.\n",
    "\n",
    "Type the name of the Lambda function you created earlier into the Lambda Function text entry box and then click on Save. Click on OK in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created.\n",
    "\n",
    "The last step in creating the API Gateway is to select the Actions dropdown and click on Deploy API. You will need to create a new Deployment stage and name it anything you like, for example `prod`.\n",
    "\n",
    "Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text Invoke URL."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "App Deployment\n",
    "\n",
    "In the `website` folder there should be a file called `index.html`. Download the file to your computer and open that file up in a text editor of your choice. There should be a line which contains **\\*\\*REPLACE WITH PUBLIC API URL\\*\\***. Replace this string with the url that you wrote down in the last step and then save the file.\n",
    "\n",
    "Now, if you open `index.html` on your local computer, your browser will behave as a local web server and you can use the provided site to interact with your SageMaker model.\n",
    "\n",
    "If you'd like to go further, you can host this html file anywhere you'd like, for example using github or hosting a static site on Amazon's S3. Once you have done this you can share the link with anyone you'd like and have them play with it too!\n",
    "\n",
    "> in order for the web app to communicate with the SageMaker endpoint, the endpoint has to actually be deployed and running. This means that you are paying for it. Make sure that the endpoint is running when you want to use the web app but that you shut it down when you don't need it, otherwise you will end up with a surprisingly large AWS bill.\n",
    "\n",
    "TODO: Make sure that you include the edited `index.html` file in your project submission.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''\n",
    "Now that your web app is working, trying playing around with it and see how well it works.\n",
    "QUESTION:\n",
    "Give an example of a review that you entered into your web app. What was the predicted sentiment of your example review?\n",
    "Review: 'The simplest pleasures in life are the best, and this film is one of them..'\n",
    "Prediction: 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}